<bug>
          <bug_id>4322</bug_id>
          
          <creation_ts>2010-11-15 18:02:00 +0100</creation_ts>
          <short_desc>Download of contributions not possible from behind a proxy requiring authentication</short_desc>
          <delta_ts>2014-03-03 08:46:25 +0100</delta_ts>
          <reporter_accessible>1</reporter_accessible>
          <cclist_accessible>1</cclist_accessible>
          <classification_id>1</classification_id>
          <classification>Unclassified</classification>
          <product>framework</product>
          <component>tool.generator</component>
          <version>unspecified</version>
          <rep_platform>All</rep_platform>
          <op_sys>All</op_sys>
          <bug_status>RESOLVED</bug_status>
          <resolution>LATER</resolution>
          
          
          
          
          
          <priority>P4</priority>
          <bug_severity>enhancement</bug_severity>
          <target_milestone>unspecified</target_milestone>
          
          
          
          <everconfirmed>1</everconfirmed>
          <reporter name="Norbert Schröder">schroeder</reporter>
          <assigned_to name="Richard Sternagel">richard.sternagel</assigned_to>
          
          <qa_contact name="qooxdoo bugs mailing list">qooxdoo-bugs</qa_contact>
          <cf_browser>---</cf_browser>
          

      

      

      

          <long_desc isprivate="0">
            <commentid>21314</commentid>
            <who name="Norbert Schröder">schroeder</who>
            <bug_when>2010-11-15 18:02:02 +0100</bug_when>
            <thetext>In its present state it is impossible for the generator to download a contribution if the client is located behind a proxy server that requires authentication. Reason: Then python module &quot;urllib&quot; is outdated in this respect and should be replaced by &quot;urllib2&quot;.

I successfully managed to download a contribution with the generator by using &quot;urllib2&quot; plus a few lines of additional authentication code in ContribLoader.getRevision and Wget.getPage, respectively:

proxy = urllib2.ProxyHandler({&#39;http&#39;: &#39;http://myuserid:mypassword@myproxy:myportnumber&#39;})
auth = urllib2.HTTPBasicAuthHandler()
opener = urllib2.build_opener(proxy, auth, urllib2.HTTPHandler)
urllib2.install_opener(opener)

To keep things as quick an dirty as possible I hard-coded myuserid, mypassword, myproxy and myportnumber. How this can be avoided in a more generic approach, I don&#39;t know.

Reference: http://bytes.com/topic/python/answers/822243-python-behind-squid-corporate-proxy-windows</thetext>
          </long_desc>
          <long_desc isprivate="0">
            <commentid>35523</commentid>
            <who name="Martin Wittemann">martin.wittemann</who>
            <bug_when>2014-03-03 08:46:25 +0100</bug_when>
            <thetext>Move open issues to RESOLVED – LATER, whose last comment is older than a year.</thetext>
          </long_desc>
      
      

    </bug>