<bug>
          <bug_id>3658</bug_id>
          
          <creation_ts>2010-04-29 15:46:00 +0200</creation_ts>
          <short_desc>Add more nightly tests for the toolchain</short_desc>
          <delta_ts>2014-03-03 08:47:29 +0100</delta_ts>
          <reporter_accessible>1</reporter_accessible>
          <cclist_accessible>1</cclist_accessible>
          <classification_id>1</classification_id>
          <classification>Unclassified</classification>
          <product>framework</product>
          <component>tool.test.automation</component>
          <version>master</version>
          <rep_platform>All</rep_platform>
          <op_sys>All</op_sys>
          <bug_status>RESOLVED</bug_status>
          <resolution>LATER</resolution>
          
          
          
          
          
          <priority>P3</priority>
          <bug_severity>enhancement</bug_severity>
          <target_milestone>unspecified</target_milestone>
          
          <blocked>2197</blocked>
          
          <everconfirmed>1</everconfirmed>
          <reporter name="Martin Wittemann">martin.wittemann</reporter>
          <assigned_to name="Richard Sternagel">richard.sternagel</assigned_to>
          <cc>qooxdoo</cc>
          <qa_contact name="qooxdoo bugs mailing list">qooxdoo-bugs</qa_contact>
          <cf_browser>---</cf_browser>
          

      

      

      

          <long_desc isprivate="0">
            <commentid>18102</commentid>
            <who name="Martin Wittemann">martin.wittemann</who>
            <bug_when>2010-04-29 15:46:27 +0200</bug_when>
            <thetext>I talked to Daniel and Thomas a few minutes ago. The current state of nightly testing for the toolchain is that some smoke tests are already done every night.

But the test coverage is far from what we want to have. Implementing some more tests should be done for 1.2.</thetext>
          </long_desc>
          <long_desc isprivate="0">
            <commentid>18748</commentid>
            <who name="Thomas Herchenroeder">thron7</who>
            <bug_when>2010-06-17 16:14:41 +0200</bug_when>
            <thetext>-&gt; unspec</thetext>
          </long_desc>
          <long_desc isprivate="0">
            <commentid>23594</commentid>
            <who name="Thomas Herchenroeder">thron7</who>
            <bug_when>2011-03-26 01:10:52 +0100</bug_when>
            <thetext>A possible approach to automatically testing compile output:

- A major obstacle to automatically testing compile output of the generator is the difficulty to verify the generated code. This mainly affects the build version with its optimizations (variant pruning, variable renaming, string extraction etc.).

- Doing textual comparisons and evaluation on the generated code, as I envisioned it before, has severe drawbacks: You have to create reference code upfront to have something to compare to, which is very tedious and a maintenance nightmare when changes to the generator affect its textual output. Yet, changes in textual output don&#39;t say much about the correctness of the generated code, or the lack thereof.

- What you actually want to test is if the generated code is *semantically* correct, that it does what it is supposed to do, independent of its textual representation. The best way to achieve that is actually running the code under a representative JS interpreter, like Rhino or node.js.

- The test code subject to such testing should ideally *verify itself*, so that running it provides sufficient evidence of the correctness of the transformations that have been applied to it. As a simple example, the code

  if (qx.core.Environment.get(&quot;foo&quot;)
    var a = 1;
  else
    var a = 2;

could be variant-optimized with the setting &quot;foo&quot;:true, and the resulting code
be executed with an &quot;assertEquals(a, 1)&quot; assertion. If the assertion holds the optimized code is correct, otherwise not. A similar test could be applied to the same code by supplying &quot;foo&quot;:false and &quot;assertEquals(a,2)&quot;.

- So the test driver would be supplied with e.g. three elements of a test: the test code, a set of config settings that stir the generator execution, and a set of assert statments that are added to the transformed code, and then executed.

This seems like a good general strategy to build upon. Maybe this can be applied not only to variant pruning. Another way of testing could be to run both the optimized and the unoptimized code and compare their outcomes.</thetext>
          </long_desc>
          <long_desc isprivate="0">
            <commentid>30892</commentid>
            <who name="Thomas Herchenroeder">thron7</who>
            <bug_when>2012-10-05 11:13:14 +0200</bug_when>
            <thetext>*** Bug 6878 has been marked as a duplicate of this bug. ***</thetext>
          </long_desc>
          <long_desc isprivate="0">
            <commentid>31364</commentid>
            <who name="Thomas Herchenroeder">thron7</who>
            <bug_when>2012-11-17 09:17:14 +0100</bug_when>
            <thetext>Parser/compiler tests:

Maybe the Ecma262 conformance tests could be used:
http://test262.ecmascript.org/</thetext>
          </long_desc>
          <long_desc isprivate="0">
            <commentid>35921</commentid>
            <who name="Martin Wittemann">martin.wittemann</who>
            <bug_when>2014-03-03 08:47:29 +0100</bug_when>
            <thetext>Move open issues to RESOLVED â€“ LATER, whose last comment is older than a year.</thetext>
          </long_desc>
      
      

    </bug>